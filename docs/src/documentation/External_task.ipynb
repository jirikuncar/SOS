{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing external tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A complex workflow usually have a large number of steps, most of them are light-weight and can be executed locally, but some of them are time- and resource-consuming and are suited to be executed on dedicated servers or cluster systems. There are several approaches in running such workflows, namely,\n",
    "\n",
    "1. Execute the entire workflow on the cluster as a single multi-processing job. This is not idea because 1). you would have to allocate enough resource for the most CPU and RAM-demanging step and longest execution time for the most time-consuming step but these resources are not utilized most of the time, and 2). you are limiting yourself to a single node and cannot really execute the workflow in parallel.\n",
    "\n",
    "2. Separate the workflow by steps and submit them as separate tasks to a queuing system. This approach allows better parallelization but it can be difficult and time consuming to execute a large number of small jobs, because these tasks might spend more time waiting than running. \n",
    "\n",
    "SoS takes a different approach than most workflow systems in that\n",
    "\n",
    "1. It executes most or all steps directly. The workflow is executed in a multi-processing manner where multiple processes (by default to 4) are used to execute different branches of the DAG (Direct Acyclic Graph).\n",
    "\n",
    "2. Part of the steps can be defined as **tasks** that are executed externally. The tasks can be executed locally as separate processes, remotely on a remote server, sent to distributed task-queues (such as [rq](http://python-rq.org/) or [Celery](http://www.celeryproject.org/), or cluster systems based on PBS, Torch, or SunGrid. SoS handles file synchronization so **tasks could be submitted to queues with their own file systems**. SoS can wait for the completion of the tasks or exits (default mode). The tasks could be executed and monitored independent of the workflows, and SoS can resume the execution of the workflow if it depends on the completion of some of the tasks.\n",
    "\n",
    "This external execution model offers great flexibility in the execution of workflows. For example,\n",
    "* You can execute a step on a remote server with more resource by specifying parameter `queue` of a single task.\n",
    "* You can submit all tasks of a workflow to a cluster by specifying cluster name with the `-q` option of command `sos run`.\n",
    "* You can submit part of the tasks to one machine, and part of the tasks to another task queue using the combination of task-specific option and global `-q` option.\n",
    "* You can use SoS as a task-generation tool to generate a bunch of tasks, and send the tasks to different computer systems for execution. SoS automatically handles file synchronization so that you can easily move from one cluster to another cluster or another server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Specification of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a job is long and time consuming, it is much preferred to submit them as separate tasks to be executed, for example, on a cluster system. These jobs should be specified using the `task` keyword, which marks the beginning of a task, with optional runtime options to control its execution. For example,\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: group_by='single'\n",
    "\n",
    "task: concurrent=True\n",
    "\n",
    "run('''\n",
    "samtools index {_input}\n",
    "''')\n",
    "```\n",
    "\n",
    "execute a shell script in parallel (with `concurrent=True`). The step process can consists of arbitrary python statements and execute multiple step actions. For example,\n",
    "\n",
    "```python\n",
    "task:\n",
    "try:\n",
    "   action1()\n",
    "except RuntimeError:\n",
    "   action2()\n",
    "```\n",
    "\n",
    "execute `action1` and `action2` if `action1` raises an error.\n",
    "\n",
    "```python\n",
    "task:\n",
    "for par in ['-4', '-6']:\n",
    "   run('command with ${par}')\n",
    "```\n",
    "\n",
    "executes commands in a loop. This is similar to\n",
    "\n",
    "```\n",
    "pars = ['-4', '-6']\n",
    "input: for_each=pars\n",
    "task:\n",
    "run('command with ${_pars}')\n",
    "```\n",
    "\n",
    "but the `for` loop version would not be able to be executed in parallel. Note that SoS actions can be used outside of `step process` but only statements specified after the `process` keyword can have runtime options and be executed in separate processes. That is to say,\n",
    "\n",
    "```\n",
    "pars = ['-4', '-6']\n",
    "input: for_each=pars\n",
    "run('command with ${_pars}')\n",
    "```\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "```\n",
    "pars = ['-4', '-6']\n",
    "input: for_each=pars\n",
    "task:\n",
    "run('command with ${_pars}')\n",
    "```\n",
    "\n",
    "but the latter can have additional runtime options to run commands in parallel\n",
    "\n",
    "```\n",
    "pars = ['-4', '-6']\n",
    "input: for_each=pars\n",
    "task: concurrent=True\n",
    "run('command with ${_pars}')\n",
    "```\n",
    "\n",
    "Because step tasks are executed outside of SoS, variables assigned in step tasks are not accessible to SoS. For example,\n",
    "\n",
    "```\n",
    "[10: shared='res']\n",
    "res = some_action()\n",
    "```\n",
    "\n",
    "executes `some_action()` in step process and return its result as a shared variable `res`. The following script,\n",
    "\n",
    "```\n",
    "[10: shared='res']\n",
    "task:\n",
    "res = some_action()\n",
    "```\n",
    "\n",
    "however, does not work because `res` is assigned in step task and is not accessible from the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Common host configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `alias`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "`alias` is the alias of the host and is used to identify a host. The hosts are usually defined in `config.yml` under `hosts` as a list of aliases.\n",
    "\n",
    "```\n",
    "hosts:\n",
    "    alias:\n",
    "        address: username@url.com\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `address`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "IP address or URL to the remote host. Name `localhost` can be used for localhost. If you have a different user name on the the remote host, specify the `address` in the format of `username@hostaddress`.\n",
    "\n",
    "Note that SoS does not support username/password authentication and you will have to set up public key authentication between local and remote hosts to  communicate with remote host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `path_map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[{\"output_type\":\"stream\",\"text\":\"File contains parsing errors: <string>\\n\\t[line  2]: `path_map` is a list of directory mappings between local and remote directories. For example, if you work locally on a Mac machine with home directory `/Users/myuser`, and the remote server is a Linux machine with home directory `/home/myuser`, you should define a `path_map` using command\\n\\n```\\n% sos config --global --set hosts.monster.path_map /Users/myuser/:/home/myuser/\\n```\\n\\nInvalid statements: SyntaxError('invalid syntax', ('<string>', 1, 1, '`path_map` is a list of directory mappings between local and remote directories. For example, if you work locally on a Mac machine with home directory `/Users/myuser`, and the remote server is a Linux machine with home directory `/home/myuser`, you should define a `path_map` using command\\\\n'))\\n\",\"name\":\"stderr\"}]"
   },
   "source": [
    "`path_map` is a list of directory mappings between local and remote directories. Paths in a `path_map` should be absolute path and a local path will be converted to absolute path before mapping. For example, a `path_map` \n",
    "\n",
    "```\n",
    "/Users/myuser/:/home/myuser/\n",
    "```\n",
    "would map `test/a.txt` under home directory to `/home/myuser/test/a.txt`, and map `/Users/myuser/resources` to `/home/myuser/resources`.\n",
    "\n",
    "Multiple `path_map` could be defined and a path is mapped by the first matching `path_map`. For example,\n",
    "\n",
    "```\n",
    "/Users/myuser/projects/:/home/myuser/scratch/projects/\n",
    "/Users/myuser:/home/myuser\n",
    "```\n",
    "\n",
    "will map `/Users/myuser/projects/input.fastq` to `/home/myuser/scratch/projects/input.fastq`, and `/users/myuser/a.txt` to `/home/myuser/a.txt`.\n",
    "\n",
    "The order of the map is significant and the second `path_map` in the following configuration will be ignored because it is shadowed by the first one.\n",
    "\n",
    "```\n",
    "/Users/myuser:/home/myuser\n",
    "/Users/myuser/projects/:/home/myuser/scratch/projects/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `shared`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `shared` tells SoS which file systems are shared between local and remote hosts so that it does not have to synchronize files under these directories between the hosts.\n",
    "\n",
    "* SoS assumes independent file systems so you do not have to specify option `shared` if the local and remote hosts does not share any file system.\n",
    " \n",
    "* If your local and remote host share all file systems, you should use command\n",
    "\n",
    "  ```bash\n",
    "  % sos config --global --set hosts.monster.shared /\n",
    "  ```\n",
    "  to indicate that the root directory is shared so no cross-network copy is needed.\n",
    "  \n",
    "* If your local and remote host share one or more shared volumes, you can specify them with command\n",
    "\n",
    "  ```bash\n",
    "  % sos config --global --set hosts.monster.shared /projects /data\n",
    "  ```\n",
    "  to indicate that files under these directories are available on the remote host.\n",
    "\n",
    "Shared file systems do not have to be mounted at the same locations. For example, a local file system `/projects` might be available at the remote host under `/scratch/projects`. In this case, you should\n",
    "\n",
    "* Set `/projects` as `shared` so that files under `/projects` will not be copied.\n",
    "* Set `/projects:/scratch/projects` in `path_map` so that the path can be correctly translated between local and remote hosts.\n",
    "\n",
    "It is important to remember that **SoS does not copy files under shared directories**. If the local and remote host share a file system but you really would like to copy files to a differenet directory, you can ignore the `shared` option and let SoS copy files as if they are separate file systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `send_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "SoS uses `rsync` command to exchange files between hosts, and use `ssh` to execute command. If the default commands do not work for your configuration (e.g. if you do not have `rsync` and need to use `scp`, you can define options `send_cmd` (and `received_cmd` and `execute_cmd`) for your particular configuration. These variables should be defined with `${source}` and `${dest}` which will be replaced by source and destination filenames for each file.\n",
    "\n",
    "It is rather tricky to define `send_cmd` for all scenarios (files, directories, missing directory on remote host) so it is usually easier to install `rsync` and use system default `send_cmd` than defining `send_cmd` by youself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `receive_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Command to receive files from remote server. It is usually easier to install `rsync` than defining this option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `execute_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Command to execute a command on remote host. The default value should work in almost all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Common queue configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `queue_type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `query_type` determines the type of remote server or job queue. SoS currently supports the following types of job queues:\n",
    "\n",
    "1. **`process`**: this is the default queue type. Tasks are executed directly, either on local host or on a server. The maximum number of concurrent processes are controlled by option `max_running_jobs`.\n",
    "2. **`pbs`**: A PBS/MOAB cluster system where tasks are submitted using commands such as `qsub`.\n",
    "3. **`rq`**: A redis queue where tasks are submitted to the rq server and monitored through rq-dashboard.\n",
    "4. **`celery`**: A celery queue where tasks are submitted to the celery server and monitored through celery's flower module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `status_check_interval`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Frequency of checking status of jobs. This is set by default to 2 seconds for `process` queue type, and `10` seconds for all other types. This number should be set to at least `60` for remote servers and longer jobs because checking remote job status involves logging to the remote server and executing a `sos status` command. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `max_running_jobs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Maximum number of running jobs. This setting controls how SoS releases tasks to job queues and is independent of possible maximum running job settings of individual task queues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Arbitrary key value pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "You can define arbitrary key value pairs in the host configuration. These variables could be used for the interpolation of commands and templates. For example, if you define\n",
    "\n",
    "```yml\n",
    "queue: long\n",
    "```\n",
    "\n",
    "You could use \n",
    "\n",
    "```bash\n",
    "#PBS -q ${queue}\n",
    "```\n",
    "\n",
    "in your PBS job templates (configuration `template_file` or `job_template`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## RQ configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `redis_host`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Address of the redis server, default to `localhost`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `redis_port`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Port of the redis server, default to `6379`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Celery configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `broker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "outputs": [],
   "source": [
    "`broker` configuration of celery app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `backend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "outputs": [],
   "source": [
    "`backend` configuration of the celery app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## PBS/Torch configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `template_file`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "`template_file` should point to the location of a template file (available locally, not on remote host) that will be used to generate a shell script that will be submitted to the PBS system. A typical template would look like\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#PBS -N ${task}\n",
    "#PBS -l nodes=${nodes}:ppn=${ppn}\n",
    "#PBS -l walltime=${walltime}\n",
    "#PBS -l mem=${mem}\n",
    "#PBS -o ${task}.out\n",
    "#PBS -e ${task}.err\n",
    "#PBS -q long\n",
    "#PBS -m ae\n",
    "#PBS -M your@email.address\n",
    "#PBS -v ${cur_dir}\n",
    "\n",
    "cd ${cur_dir}\n",
    "\n",
    "sos execute ${task} -v ${verbosity} -s ${sig_mode}\n",
    "```\n",
    "\n",
    "The template file will be interpolated with the following information\n",
    "\n",
    "* `task`: task id\n",
    "* `nodes`, `ppn`, `walltime`, `mem`: resource task options\n",
    "* `cur_dir`: translated current project directory\n",
    "* `verbosity` and `sig_mode`: sos run mode.\n",
    "\n",
    "Note that\n",
    "1. You will need to specify all resource options (`nodes`, `ppn`, `walltime`, and `mem`) as task options if they are used in the template file.\n",
    "2. If you need to specify more options (e.g. queue name), you will have to define multiple host entries with different template files. For example, you could define two queue entries as `cluster-short` and `cluster-long` for two queues on the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `job_template`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "`job_template` should be the content of the template file if you prefer listing the content directly in the config file, and happen to know how to specify multi-line strings in YAML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `submit_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A `submit_cmd` template is the command that will be executed to submit the job. It accepts the same set of variables as `job_template`, with an additional variable `job_file` pointing to the location of the job file on the remote host. The `submit_cmd` is usually as simple as\n",
    "\n",
    "```bash\n",
    "qsub ${job_file}\n",
    "```\n",
    "\n",
    "but you could specify some options from command line instead of the job file and define `submit_cmd` as\n",
    "\n",
    "```bash\n",
    "msub -l ${walltime} < ${job_file}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `status_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A command to query the status of a submitted task. For a standard PBS system, this option could be\n",
    "\n",
    "```\n",
    "qstat ${job_id}\n",
    "```\n",
    "\n",
    "where `job_id` is the output of command `submit_cmd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `kill_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A command to kill a submitted job on the cluster. For a standard PBS system, this option could be\n",
    "\n",
    "```\n",
    "qdel ${job_id}\n",
    "```\n",
    "where `job_id` is the output of command `submit_cmd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Resource options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The resource options will be sent to individual task queues in appropriate format. You do not have to specify all options because task queues can support a subset of these options and some task queues provide default values (and some do not). It is however generally a good idea to specify them all so that your tasks could be executed on all types of task queues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `walltime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Estimated maximum running time of the task. This parameter will be sent to different task queues and it is up to the task queue to decide if the task would be killed if the task could not be completed within specified `walltime`. `walltime` could be specified\n",
    "\n",
    "1. As an integer number (in seconds).\n",
    "2. As a string in the format of `HH:MM:SS` where `HH`, `MM` and `SS` are hours, minutes, and seconds. For example, you could use `walltime=240:00:00` for a job that would run 10 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `nodes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Number of computing nodes requested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `ppn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Number of processes on each computing node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `mem`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The total amount of memory needed across all nodes. Default units are bytes; can also be expressed in megabytes (mem=4000MB) or gigabytes (mem=4GB). This option can be specified as a number (bytes) or string such as `4GB`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Execution options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `queue`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `queue` specifies a task queue to which the current task will be submitted. This option overrides system default (command line option `-q`) so it is generally a good idea to use command line option `-q` so that the task could be submitted to different task queues, unless the task has to be executed in a particular server (e.g. with a software that is unavailable elsewhere)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `preserved_vars`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "All variables in the task context environment, including implicit SoS variables such as `_input` and `_output` are automatically translated if they are string or sequence of strings (list, tuple etc). This would however translate variables such as `sample_name` with value `my_sample` to something like `/home/myuser/project/my_sample`. It is therefore important for you to identify all variables that should be preserved during context-switching in the option `preserved_vars`.\n",
    "\n",
    "This variables takes a list of variable names, but a string can be specified if there is only one name. That is to say, both\n",
    "\n",
    "```\n",
    "preserved_vars='sample_name'\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "preserved_vars=['sample_name', 'title']\n",
    "```\n",
    "\n",
    "are acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `workdir`\n",
    "\n",
    "Default to current working directory.\n",
    "\n",
    "Option `workdir` controls the working directory of the process. For example, the following step downloads a file to the `resource_dir` using command `wget`.\n",
    "\n",
    "```python\n",
    "[10]\n",
    "\n",
    "run: workdir=resource_dir\n",
    "\n",
    "  wget a_url -O filename\n",
    "\n",
    "```\n",
    "\n",
    "Runtime option `workdir` will be translated to remote host if the task is executed remotely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `concurrent`\n",
    "\n",
    "Default to `False`.\n",
    "\n",
    "If the step process is repeated for different input files or parameters (using input options `group_by` or `for_each`), the loop process can be execute in parallel, up to the maximum number of concurrent jobs specified by `max_running_jobs` option of the task queue.\n",
    "\n",
    "### Option `env`\n",
    "\n",
    "The `env` option allow you to modify runtime environment, similar to the `env` parameter of the `subprocess.Popen` function. For example, you can execute your command with in a specific directory using\n",
    "\n",
    "```sos\n",
    "task:  env={'PATH': '/path/to/mycommand' + os.sep + os.environ['PATH']}\n",
    "run:\n",
    "   mycommand \n",
    "```\n",
    "\n",
    "Option `env` is NOT translated to remote host because it is of type directionay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `prepend_path`\n",
    "\n",
    "Option `prepend_path` is a shortcut to option `env` to prepend one (a string) or more (a list of strings) paths to system path. For example, the above example can be shortened to\n",
    "\n",
    "```sos\n",
    "task:  prepend_path='/path/to/mycommand'\n",
    "run:\n",
    "   mycommand \n",
    "```\n",
    "\n",
    "Option `prepend_path` is NOT translated to remote host because it is likely to be host specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `active`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Option `active` specifies the active task within a input loop. It should be an index or a list of indexes when the task will be executed. Negative index is acceptable (e.g. task for only the last input loop will be executed with `active=-1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Commands and Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos run -q`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "The `-q` option of command `sos run` (or `sos-runner`) sets the default task queue for all tasks. For example,\n",
    "\n",
    "```bash\n",
    "sos run myscript -q shark\n",
    "```\n",
    "\n",
    "would send all tasks in workflow `default` defined in `myscript.sos` to a task queue `shark`, with detailed information about `shark` defined in either global `~/.sos/config.yml` or local (`./config.yml`) formats. You can also save configurations to other configuration files and specify them using option `-c`. E.g.\n",
    "\n",
    "```bash\n",
    "sos run myscript -q shark -c shark.yml\n",
    "```\n",
    "\n",
    "Note that this option does not override option `queue` of steps so you could send some tasks to specific queues and all others to the default queue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos status`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Command \n",
    "\n",
    "```bash\n",
    "sos status [tasks] -q query\n",
    "```\n",
    "checkes the status of tasks. You can specify any number of first characeters of a task to specify a task, for example,\n",
    "\n",
    "```bash\n",
    "sos task 7\n",
    "sos task 77e\n",
    "sos task 7736e\n",
    "```\n",
    "would all work for a task with ID `77e36e7404cf6c2ef7079a09e84a4d6d`, but multiple tasks could be identifies if they share the same leading digits. Actually, \n",
    "\n",
    "```\n",
    "sos task \n",
    "```\n",
    "would match all tasks and list the status of all local tasks.\n",
    "\n",
    "Option `-q` specifies the task queue to monitor. It will login to a remote host if the tasks are executed on a remote server. For example,\n",
    "\n",
    "```\n",
    "sos status -q docker\n",
    "```\n",
    "\n",
    "would check the status of all tasks on a remote host `docker`.\n",
    "\n",
    "Option `-v` controls the details of the output of command `sos status`. For example,\n",
    "\n",
    "```\n",
    "sos status e7404cf6c2 -v0\n",
    "```\n",
    "would print just the status of the task (e.g. `running`).\n",
    "\n",
    "```\n",
    "sos task 77e -v1\n",
    "```\n",
    "would print the task id and their status\n",
    "\n",
    "```\n",
    "77e36e7404cf6c2ef7079a09e84a4d6d    running\n",
    "77e3c2ef7079a236e7404cf6c2f343d3    completed\n",
    "```\n",
    "\n",
    "Option `-v0` and `-v1` could check the status of multiple tasks, as realized by SoS. Some tasks queues have their own task status command and option `-v2` (and upper) will use these commands (if specified) to check the status of the jobs. That is to say\n",
    "\n",
    "``` bash\n",
    "sos task 77e36 -v2\n",
    "```\n",
    "\n",
    "might return output of a command\n",
    "\n",
    "```\n",
    "qstat 18433 -q shark\n",
    "```\n",
    "\n",
    "if the task has been submitted to a cluster named `shark` with a job id `18433`.\n",
    "\n",
    "If you would like to know more about the tasks,\n",
    "\n",
    "```bash\n",
    "sos task 77e36 -v3\n",
    "```\n",
    "\n",
    "would list the script the task is running and all variables in abbreviated format, and\n",
    "\n",
    "```bash\n",
    "sos task 77e36 -v4\n",
    "```\n",
    "would list all variables in complete form.\n",
    "\n",
    "Finally, using `-q` in combination with `-v` allows you to list the variables used in remote server.\n",
    "\n",
    "```bash\n",
    "sos task 77e36 -v4 -q linux\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos kill` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Command\n",
    "\n",
    "```bash\n",
    "sos kill [tasks] [-q queue]\n",
    "```\n",
    "\n",
    "kills specified or all tasks on specified job queue `queue`. Because the same job could be executed on different queues (you have have done so), you will have to specify the correct queue name to kill the job on different queues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos execute`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "Command \n",
    "````\n",
    "sos execute [tasks] [-q queue]\n",
    "```\n",
    "\n",
    "is the command that is used internally by `sos run` to execute tasks but you could use this command to execute tasks externally. For example, if a task failed on a server, you could use command\n",
    "\n",
    "```\n",
    "sos execute task_id -q server\n",
    "```\n",
    "\n",
    "to execute the command on another server. Note that `task_id` specifies a local task with local paths. The task will be converted to a remote task (with path names converted for that host) if `server` specifies a remote host. This makes it easy for you to re-submit tasks to the same server after changing server configuration, or submit the same task to a different server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "source": [
    "### Remote execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### PBS Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "source": [
    "Host configuration:  `~/.sos/config.yml`\n",
    "\n",
    "```yml\n",
    "hosts:\n",
    "  nautilus:\n",
    "    address: mdarisngc03.mdanderson.edu\n",
    "    path_map:\n",
    "    - /Users/bpeng1:/scratch/bcb/bpeng1\n",
    "    queue_type: pbs\n",
    "    status_check_interval: 30\n",
    "    template_file: ~/.sos/HPC.tmpl\n",
    "    max_running_jobs: 100\n",
    "    submit_cmd: msub ${job_file}\n",
    "    status_cmd: qstat ${job_id}\n",
    "    kill_cmd: qdel ${job_id}\n",
    "```\n",
    "\n",
    "Test script:\n",
    "\n",
    "```sos\n",
    "[10]\n",
    "input: for_each={'tid': range(10) }\n",
    "\n",
    "task: concurrent=True, walltime='00:20:00', mem='100M', nodes=1, ppn=1\n",
    "\n",
    "run:\n",
    "    echo I am task ${tid}\n",
    "    # sleep 1 \n",
    "    sleep ${60  * (tid + 1)}\n",
    "\n",
    "```\n",
    "\n",
    "Commands:\n",
    "\n",
    "```\n",
    "sos run test -q nautilus\n",
    "sos status -q nautilus\n",
    "sos kill cb1 -q nautilus\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos.jupyter.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "celltoolbar": true,
   "kernels": [
    [
     "sos",
     "SoS",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   }
  },
  "toc": {
   "nav_menu": {
    "height": "172px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
