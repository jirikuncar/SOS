{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing external tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A complex workflow usually have a large number of steps, most of them are light-weight and can be executed locally, but some of them are time- and resource-consuming and are suited to be executed on dedicated servers or cluster systems. There are several approaches in running such workflows, namely,\n",
    "\n",
    "1. Execute the entire workflow on the cluster as a single multi-processing job. This is not idea because 1). you would have to allocate enough resource for the most CPU and RAM-demanging step and longest execution time for the most time-consuming step but these resources are not utilized most of the time, and 2). you are limiting yourself to a single node and cannot really execute the workflow in parallel.\n",
    "\n",
    "2. Separate the workflow by steps and submit them as separate tasks to a queuing system. This approach allows better parallelization but it can be difficult and time consuming to execute a large number of small jobs, because these tasks might spend more time waiting than running. \n",
    "\n",
    "SoS takes a different approach than most workflow systems in that\n",
    "\n",
    "1. It executes most or all steps directly. The workflow is executed in a multi-processing manner where multiple processes (by default to 4) are used to execute different branches of the DAG (Direct Acyclic Graph).\n",
    "\n",
    "2. Part of the steps can be defined as **tasks** that are executed externally. The tasks can be executed locally as separate processes, remotely on a remote server, sent to distributed task-queues (such as [rq](http://python-rq.org/) or [Celery](http://www.celeryproject.org/), or cluster systems based on PBS, Torch, or SunGrid. SoS handles file synchronization so **tasks could be submitted to queues with their own file systems**. SoS can wait for the completion of the tasks or exits (default mode). The tasks could be executed and monitored independent of the workflows, and SoS can resume the execution of the workflow if it depends on the completion of some of the tasks.\n",
    "\n",
    "This external execution model offers great flexibility in the execution of workflows. For example,\n",
    "* You can execute a step on a remote server with more resource by specifying parameter `queue` of a single task.\n",
    "* You can submit all tasks of a workflow to a cluster by specifying cluster name with the `-q` option of command `sos run`.\n",
    "* You can submit part of the tasks to one machine, and part of the tasks to another task queue using the combination of task-specific option and global `-q` option.\n",
    "* You can use SoS as a task-generation tool to generate a bunch of tasks, and send the tasks to different computer systems for execution. SoS automatically handles file synchronization so that you can easily move from one cluster to another cluster or another server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Specification of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a job is long and time consuming, it is much preferred to submit them as separate tasks to be executed, for example, on a cluster system. These jobs should be specified using the `task` keyword, which marks the beginning of a task, with optional runtime options to control its execution. For example,\n",
    "\n",
    "```\n",
    "[10]\n",
    "input: group_by='single'\n",
    "\n",
    "task: concurrent=True\n",
    "\n",
    "run('''\n",
    "samtools index {_input}\n",
    "''')\n",
    "```\n",
    "\n",
    "execute a shell script in parallel (with `concurrent=True`). The step process can consists of arbitrary python statements and execute multiple step actions. For example,\n",
    "\n",
    "```python\n",
    "task:\n",
    "try:\n",
    "   action1()\n",
    "except RuntimeError:\n",
    "   action2()\n",
    "```\n",
    "\n",
    "execute `action1` and `action2` if `action1` raises an error.\n",
    "\n",
    "```python\n",
    "task:\n",
    "for par in ['-4', '-6']:\n",
    "   run('command with ${par}')\n",
    "```\n",
    "\n",
    "executes commands in a loop. This is similar to\n",
    "\n",
    "```\n",
    "pars = ['-4', '-6']\n",
    "input: for_each=pars\n",
    "task:\n",
    "run('command with ${_pars}')\n",
    "```\n",
    "\n",
    "but the `for` loop version would not be able to be executed in parallel. Note that SoS actions can be used outside of `step process` but only statements specified after the `process` keyword can have runtime options and be executed in separate processes. That is to say,\n",
    "\n",
    "```\n",
    "pars = ['-4', '-6']\n",
    "input: for_each=pars\n",
    "run('command with ${_pars}')\n",
    "```\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "```\n",
    "pars = ['-4', '-6']\n",
    "input: for_each=pars\n",
    "task:\n",
    "run('command with ${_pars}')\n",
    "```\n",
    "\n",
    "but the latter can have additional runtime options to run commands in parallel\n",
    "\n",
    "```\n",
    "pars = ['-4', '-6']\n",
    "input: for_each=pars\n",
    "task: concurrent=True\n",
    "run('command with ${_pars}')\n",
    "```\n",
    "\n",
    "Because step tasks are executed outside of SoS, variables assigned in step tasks are not accessible to SoS. For example,\n",
    "\n",
    "```\n",
    "[10: shared='res']\n",
    "res = some_action()\n",
    "```\n",
    "\n",
    "executes `some_action()` in step process and return its result as a shared variable `res`. The following script,\n",
    "\n",
    "```\n",
    "[10: shared='res']\n",
    "task:\n",
    "res = some_action()\n",
    "```\n",
    "\n",
    "however, does not work because `res` is assigned in step task and is not accessible from the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Common host configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `alias`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `address`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `path_map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `shared`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `send_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `receive_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `execute_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Common queue configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `queue_type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `status_check_interval`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `max_running_jobs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## RQ configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `redis_host`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `redis_port`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Celery configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `broker`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `backend`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## PBS/Torch configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `template_file`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "`template_file` should point to the location of a template file (available locally, not on remote host) that will be used to generate a shell script that will be submitted to the PBS system. A typical template would look like\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "$PBS -N ${task}\n",
    "#PBS -l nodes=${nodes}:ppn=${ppn}\n",
    "#PBS -l walltime=${walltime}\n",
    "#PBS -l mem=${mem}\n",
    "#PBS -o ${task}.out\n",
    "#PBS -e ${task}.err\n",
    "#PBS -q long\n",
    "#PBS -m ae\n",
    "#PBS -M your@email.address\n",
    "#PBS -v ${cur_dir}\n",
    "\n",
    "cd ${cur_dir}\n",
    "\n",
    "source /setup.sh\n",
    "sos execute ${task} -v ${verbosity} -s ${sig_mode}\n",
    "```\n",
    "\n",
    "The template file will be interpolated with the following information\n",
    "\n",
    "* `task`: task id\n",
    "* `nodes`, `ppn`, `walltime`, `mem`: resource task options\n",
    "* `cur_dir`: translated current project directory\n",
    "* `verbosity` and `sig_mode`: sos run mode.\n",
    "\n",
    "Note that\n",
    "1. You will need to specify all resource options (`nodes`, `ppn`, `walltime`, and `mem`) as task options if they are used in the template file.\n",
    "2. If you need to specify more options (e.g. queue name), you will have to define multiple host entries with different template files. For example, you could define two queue entries as `cluster-short` and `cluster-long` for two queues on the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `job_template`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "`job_template` should be the content of the template file if you prefer listing the content directly in the config file, and happen to know how to specify multi-line strings in YAML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `submit_cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "A `submit_cmd` template is the command that will be executed to submit the job. It accepts the same set of variables as `job_template`, with an additional variable `job_file` pointing to the location of the job file on the remote host. The `submit_cmd` is usually as simple as\n",
    "\n",
    "```\n",
    "qsub ${job_file}\n",
    "```\n",
    "\n",
    "but you could specify some options from command line instead of the job file and define `submit_cmd` as\n",
    "\n",
    "```\n",
    "msub -l ${walltime} < ${job_file}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `status_cmd` (unused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "This option is currently unused because sos provides its own job monitoring method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `kill_cmd` (unused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "This option is currently unused because sos provides its own job management method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Resource options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `walltime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `nodes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `ppn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `mem`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Execution options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `queue`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `workdir`\n",
    "\n",
    "Default to current working directory.\n",
    "\n",
    "Option `workdir` controls the working directory of the process. For example, the following step downloads a file to the `resource_dir` using command `wget`.\n",
    "\n",
    "```python\n",
    "[10]\n",
    "\n",
    "run: workdir=resource_dir\n",
    "\n",
    "  wget a_url -O filename\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `concurrent`\n",
    "\n",
    "Default to `False`.\n",
    "\n",
    "If the step process is repeated for different input files or parameters (using input options `group_by` or `for_each`), the loop process can be execute in parallel, up to the maximum number of concurrent jobs specified by command line option `-j`.\n",
    "\n",
    "### Option `env`\n",
    "\n",
    "The `env` option allow you to modify runtime environment, similar to the `env` parameter of the `subprocess.Popen` function. For example, you can execute your command with in a specific directory using\n",
    "\n",
    "```\n",
    "task:  env={'PATH': '/path/to/mycommand' + os.sep + os.environ['PATH']}\n",
    "run:\n",
    "   mycommand \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option `prepend_path`\n",
    "\n",
    "Option `prepend_path` is a shortcut to option `env` to prepend one (a string) or more (a list of strings) paths to system path. For example, the above example can be shortened to\n",
    "\n",
    "```\n",
    "task:  prepend_path='/path/to/mycommand'\n",
    "run:\n",
    "   mycommand \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### Option `active`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "## Commands and Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos run -q`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos status`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "`sos status -q`\n",
    "\n",
    "`sos status -v 3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos kill` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "`sos kill -q`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### `sos execute`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "`sos execute -q`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "source": [
    "### Remote execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "output_cache": "[]"
   },
   "source": [
    "### PBS Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "output_cache": "[]"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos.jupyter.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "celltoolbar": true,
   "kernels": [
    [
     "sos",
     "SoS",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   }
  },
  "toc": {
   "nav_menu": {
    "height": "172px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
